{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from dequant_gptq import get_pytorch_bin\n",
    "import torch\n",
    "import os\n",
    "cur_dir = os.getcwd()\n",
    "deq_model_dir = 'dequant_model'\n",
    "opt = get_pytorch_bin('/home/jwjeong/.cache/huggingface/hub/models--jwjeong--opt-6.7b-4bit-128g/snapshots/1ea4ce8577b3993ebca6a7308c04a6ec72a4148b/gptq_model-4bit-128g.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_minmax(state_dict):\n",
    "    linear_opt = [\n",
    "        'k_proj',\n",
    "        'q_proj',\n",
    "        'v_proj',\n",
    "        'out_proj',\n",
    "        'fc1',\n",
    "        'fc2'\n",
    "    ]\n",
    "    layer_weight_min = {}\n",
    "    layer_weight_max = {}\n",
    "    for key in state_dict.keys():\n",
    "        if key.split('.')[-2] in linear_opt:\n",
    "            tensor = state_dict[key]\n",
    "            layer_num = int(key.split('.')[3])\n",
    "            if layer_num in layer_weight_min:\n",
    "                layer_weight_min[layer_num] = tensor.min() if tensor.min() < layer_weight_min[layer_num] else layer_weight_min[layer_num]\n",
    "                layer_weight_max[layer_num] = tensor.max() if tensor.max() > layer_weight_max[layer_num] else layer_weight_max[layer_num]\n",
    "            else:\n",
    "                layer_weight_min[layer_num] = tensor.min()\n",
    "                layer_weight_max[layer_num] = tensor.max()\n",
    "\n",
    "    print(f'layer_max: {layer_weight_max}')\n",
    "    print(f'layer_min: {layer_weight_min}')\n",
    "\n",
    "    y_min = []\n",
    "    y_max = []\n",
    "    for key in layer_weight_min.keys():\n",
    "        y_min.append(layer_weight_min[key])\n",
    "        y_max.append(layer_weight_max[key])\n",
    "\n",
    "    x = list(range(len(y_min)))\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(x, y_min)\n",
    "    plt.plot(x, y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_max: {0: tensor(0.9390, dtype=torch.float16), 1: tensor(0.9424, dtype=torch.float16), 2: tensor(0.9331, dtype=torch.float16), 3: tensor(0.7979, dtype=torch.float16), 4: tensor(0.7520, dtype=torch.float16), 5: tensor(0.7324, dtype=torch.float16), 6: tensor(0.6538, dtype=torch.float16), 7: tensor(1., dtype=torch.float16), 8: tensor(1., dtype=torch.float16), 9: tensor(1., dtype=torch.float16), 10: tensor(1., dtype=torch.float16), 11: tensor(1.4219, dtype=torch.float16)}\n",
      "layer_min: {0: tensor(-1.1582, dtype=torch.float16), 1: tensor(-1.2744, dtype=torch.float16), 2: tensor(-1.2100, dtype=torch.float16), 3: tensor(-1.1299, dtype=torch.float16), 4: tensor(-1.1143, dtype=torch.float16), 5: tensor(-1.1084, dtype=torch.float16), 6: tensor(-0.9414, dtype=torch.float16), 7: tensor(-1., dtype=torch.float16), 8: tensor(-1., dtype=torch.float16), 9: tensor(-1., dtype=torch.float16), 10: tensor(-1., dtype=torch.float16), 11: tensor(-1.2773, dtype=torch.float16)}\n",
      "layer_max: {0: tensor(0.9390, dtype=torch.float16), 1: tensor(0.9424, dtype=torch.float16), 2: tensor(0.9331, dtype=torch.float16), 3: tensor(0.7979, dtype=torch.float16), 4: tensor(0.7520, dtype=torch.float16), 5: tensor(0.7324, dtype=torch.float16), 6: tensor(0.6538, dtype=torch.float16), 7: tensor(1., dtype=torch.float16), 8: tensor(1., dtype=torch.float16), 9: tensor(1., dtype=torch.float16), 10: tensor(1., dtype=torch.float16), 11: tensor(1.4219, dtype=torch.float16)}\n",
      "layer_min: {0: tensor(-1.1582, dtype=torch.float16), 1: tensor(-1.2744, dtype=torch.float16), 2: tensor(-1.2100, dtype=torch.float16), 3: tensor(-1.1299, dtype=torch.float16), 4: tensor(-1.1143, dtype=torch.float16), 5: tensor(-1.1084, dtype=torch.float16), 6: tensor(-0.9414, dtype=torch.float16), 7: tensor(-1., dtype=torch.float16), 8: tensor(-1., dtype=torch.float16), 9: tensor(-1., dtype=torch.float16), 10: tensor(-1., dtype=torch.float16), 11: tensor(-1.2773, dtype=torch.float16)}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_minmax(orig_model.state_dict())\n",
    "plot_minmax(quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_max: {0: tensor(0.9390, dtype=torch.float16), 1: tensor(0.9424, dtype=torch.float16), 2: tensor(0.9331, dtype=torch.float16), 3: tensor(0.7979, dtype=torch.float16), 4: tensor(0.7520, dtype=torch.float16), 5: tensor(0.7324, dtype=torch.float16), 6: tensor(0.6538, dtype=torch.float16), 7: tensor(1., dtype=torch.float16), 8: tensor(1., dtype=torch.float16), 9: tensor(1., dtype=torch.float16), 10: tensor(1., dtype=torch.float16), 11: tensor(1.4219, dtype=torch.float16)}\n",
      "layer_min: {0: tensor(-1.1582, dtype=torch.float16), 1: tensor(-1.2744, dtype=torch.float16), 2: tensor(-1.2100, dtype=torch.float16), 3: tensor(-1.1299, dtype=torch.float16), 4: tensor(-1.1143, dtype=torch.float16), 5: tensor(-1.1084, dtype=torch.float16), 6: tensor(-0.9414, dtype=torch.float16), 7: tensor(-1., dtype=torch.float16), 8: tensor(-1., dtype=torch.float16), 9: tensor(-1., dtype=torch.float16), 10: tensor(-1., dtype=torch.float16), 11: tensor(-1.2773, dtype=torch.float16)}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jwjeong/anaconda3/envs/custom/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "\n",
    "repo_llama = 'TheBloke/Llama-2-7B-GPTQ'\n",
    "repo_relu_llama = 'jwjeong/ReluLLaMA-7B-4bit-gptq-128g'\n",
    "repo_opt_7b = 'jwjeong/opt-6.7b-4bit-128g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - You passed a model that is compatible with the Marlin int4*fp16 GPTQ kernel but use_marlin is False. We recommend using `use_marlin=True` to use the optimized Marlin kernels for inference. Example: `model = AutoGPTQForCausalLM.from_quantized(..., use_marlin=True)`.\n",
      "INFO - The layer lm_head is not quantized.\n",
      "Skipping module injection for FusedLlamaMLPForQuantizedModel as currently not supported with use_triton=False.\n"
     ]
    }
   ],
   "source": [
    "model_llama = AutoGPTQForCausalLM.from_quantized(repo_llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "qweight = model_llama.model.model.layers[0].mlp.down_proj.qweight\n",
    "scales = model_llama.model.model.layers[0].mlp.down_proj.scales\n",
    "qzeros = model_llama.model.model.layers[0].mlp.down_proj.qzeros\n",
    "g_idx = model_llama.model.model.layers[0].mlp.down_proj.g_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1376, 4096])\n",
      "torch.Size([86, 4096])\n",
      "torch.Size([86, 512])\n",
      "torch.Size([11008])\n"
     ]
    }
   ],
   "source": [
    "print(qweight.size())\n",
    "print(scales.size())\n",
    "print(qzeros.size()) # 4096 / \n",
    "print(g_idx.size()) # 86 * 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 725/725 [00:00<00:00, 312kB/s]\n",
      "quantize_config.json: 100%|██████████| 211/211 [00:00<00:00, 238kB/s]\n",
      "gptq_model-4bit-128g.safetensors: 100%|██████████| 3.90G/3.90G [02:24<00:00, 27.0MB/s]\n",
      "INFO - The layer lm_head is not quantized.\n",
      "Skipping module injection for FusedLlamaMLPForQuantizedModel as currently not supported with use_triton=False.\n"
     ]
    }
   ],
   "source": [
    "model_relu_llama = AutoGPTQForCausalLM.from_quantized(repo_relu_llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gptq_model-4bit-128g.safetensors: 100%|██████████| 4.20G/4.20G [01:48<00:00, 38.6MB/s]\n",
      "INFO - The layer lm_head is not quantized.\n",
      "WARNING - OPTGPTQForCausalLM hasn't fused attention module yet, will skip inject fused attention.                \n",
      "WARNING - OPTGPTQForCausalLM hasn't fused mlp module yet, will skip inject fused mlp.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.13 GiB (GPU 4; 3.81 GiB total capacity; 3.58 GiB already allocated; 120.00 MiB free; 3.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_opt_7b \u001b[38;5;241m=\u001b[39m \u001b[43mAutoGPTQForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_quantized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_opt_7b\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/custom/AutoGPTQ/auto_gptq/modeling/auto.py:133\u001b[0m, in \u001b[0;36mAutoGPTQForCausalLM.from_quantized\u001b[0;34m(cls, model_name_or_path, device_map, max_memory, device, low_cpu_mem_usage, use_triton, inject_fused_attention, inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors, trust_remote_code, warmup_triton, trainable, disable_exllama, disable_exllamav2, use_marlin, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# TODO: do we need this filtering of kwargs? @PanQiWei is there a reason we can't just pass all kwargs?\u001b[39;00m\n\u001b[1;32m    128\u001b[0m keywords \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    129\u001b[0m     key: kwargs[key]\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(signature(quant_func)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m+\u001b[39m huggingface_kwargs\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m kwargs\n\u001b[1;32m    132\u001b[0m }\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquant_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_triton\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_triton\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43minject_fused_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minject_fused_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43minject_fused_mlp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minject_fused_mlp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cuda_fp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cuda_fp16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantize_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantize_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_basename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_basename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_triton\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_triton\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_exllama\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_exllama\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_exllamav2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_exllamav2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_marlin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_marlin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkeywords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/custom/AutoGPTQ/auto_gptq/modeling/_base.py:1339\u001b[0m, in \u001b[0;36mBaseGPTQForCausalLM.from_quantized\u001b[0;34m(cls, model_name_or_path, device_map, max_memory, device, low_cpu_mem_usage, use_triton, use_qigen, use_marlin, torch_dtype, inject_fused_attention, inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors, trust_remote_code, warmup_triton, trainable, disable_exllama, disable_exllamav2, **kwargs)\u001b[0m\n\u001b[1;32m   1336\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfused_mlp_module_type\u001b[38;5;241m.\u001b[39minject_to_model(model, use_triton\u001b[38;5;241m=\u001b[39muse_triton)\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;66;03m# Any post-initialization that require device information, for example buffers initialization on device.\u001b[39;00m\n\u001b[0;32m-> 1339\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mautogptq_post_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_act_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantize_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc_act\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1341\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;66;03m# == step6: (optional) warmup triton == #\u001b[39;00m\n",
      "File \u001b[0;32m~/custom/AutoGPTQ/auto_gptq/modeling/_utils.py:492\u001b[0m, in \u001b[0;36mautogptq_post_init\u001b[0;34m(model, use_act_order, max_input_length)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(submodule, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQUANT_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m submodule\u001b[38;5;241m.\u001b[39mQUANT_TYPE \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexllamav2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    491\u001b[0m             device \u001b[38;5;241m=\u001b[39m submodule\u001b[38;5;241m.\u001b[39mqweight\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 492\u001b[0m             \u001b[43msubmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_dq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_tensors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/custom/AutoGPTQ/auto_gptq/nn_modules/qlinear/qlinear_exllamav2.py:178\u001b[0m, in \u001b[0;36mQuantLinear.post_init\u001b[0;34m(self, temp_dq)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqweight\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_tensors \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqweight\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqweight,\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqzeros,\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscales\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscales,\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mg_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_idx,\n\u001b[1;32m    177\u001b[0m }\n\u001b[0;32m--> 178\u001b[0m temp_dq \u001b[38;5;241m=\u001b[39m \u001b[43mtemp_dq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_scratch_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemp_dq_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_handle \u001b[38;5;241m=\u001b[39m ext_make_q_matrix(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_tensors, temp_dq)\n",
      "File \u001b[0;32m~/custom/AutoGPTQ/auto_gptq/nn_modules/qlinear/qlinear_exllamav2.py:224\u001b[0m, in \u001b[0;36mExLlamaV2DeviceTensors.get_scratch_slice\u001b[0;34m(self, size_bytes)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_scratch_slice\u001b[39m(\u001b[38;5;28mself\u001b[39m, size_bytes):\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscratch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 224\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m     size_bytes \u001b[38;5;241m=\u001b[39m ((size_bytes \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m127\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m128\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m    227\u001b[0m     size_half \u001b[38;5;241m=\u001b[39m size_bytes \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/custom/AutoGPTQ/auto_gptq/nn_modules/qlinear/qlinear_exllamav2.py:216\u001b[0m, in \u001b[0;36mExLlamaV2DeviceTensors.prepare\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscratch \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscratch_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_torch_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.13 GiB (GPU 4; 3.81 GiB total capacity; 3.58 GiB already allocated; 120.00 MiB free; 3.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model_opt_7b = AutoGPTQForCausalLM.from_quantized(repo_opt_7b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
